{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"x",
				"xk"
			]
		]
	},
	"buffers":
	[
		{
			"contents": "%% \n%% Copyright 2007, 2008, 2009 Elsevier Ltd\n%% \n%% This file is part of the 'Elsarticle Bundle'.\n%% ---------------------------------------------\n%% \n%% It may be distributed under the conditions of the LaTeX Project Public\n%% License, either version 1.2 of this license or (at your option) any\n%% later version.  The latest version of this license is in\n%%    http://www.latex-project.org/lppl.txt\n%% and version 1.2 or later is part of all distributions of LaTeX\n%% version 1999/12/01 or later.\n%% \n%% The list of all files belonging to the 'Elsarticle Bundle' is\n%% given in the file `manifest.txt'.\n%% \n\n%% Template article for Elsevier's document class `elsarticle'\n%% with numbered style bibliographic references\n%% SP 2008/03/01\n\n\\documentclass[preprint,12pt]{elsarticle}\n\n%% Use the option review to obtain double line spacing\n%% \\documentclass[authoryear,preprint,review,12pt]{elsarticle}\n\n%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn\n%% for a journal layout:\n%% \\documentclass[final,1p,times]{elsarticle}\n%% \\documentclass[final,1p,times,twocolumn]{elsarticle}\n%% \\documentclass[final,3p,times]{elsarticle}\n%% \\documentclass[final,3p,times,twocolumn]{elsarticle}\n%% \\documentclass[final,5p,times]{elsarticle}\n%% \\documentclass[final,5p,times,twocolumn]{elsarticle}\n\n%% For including figures, graphicx.sty has been loaded in\n%% elsarticle.cls. If you prefer to use the old commands\n%% please give \\usepackage{epsfig}\n\n%% The amssymb package provides various useful mathematical symbols\n\\usepackage{amssymb}\n%% The amsthm package provides extended theorem environments\n%% \\usepackage{amsthm}\n\n%% The lineno packages adds line numbers. Start line numbering with\n%% \\begin{linenumbers}, end it with \\end{linenumbers}. Or switch it on\n%% for the whole article with \\linenumbers.\n\\usepackage{lineno}\n\\usepackage{algorithm}\n\\usepackage{algorithmic}\n\\usepackage{graphicx}\n\\usepackage{subcaption}\n\n\\def\\bibsection{\\section*{References}}\n\n\\journal{Future Generation Computer Systems}\n\n\\begin{document}\n\n\\begin{frontmatter}\n\n%% Title, authors and addresses\n\n%% use the tnoteref command within \\title for footnotes;\n%% use the tnotetext command for theassociated footnote;\n%% use the fnref command within \\author or \\address for footnotes;\n%% use the fntext command for theassociated footnote;\n%% use the corref command within \\author for corresponding author footnotes;\n%% use the cortext command for theassociated footnote;\n%% use the ead command for the email address,\n%% and the form \\ead[url] for the home page:\n%% \\title{Title\\tnoteref{label1}}\n%% \\tnotetext[label1]{}\n%% \\author{Name\\corref{cor1}\\fnref{label2}}\n%% \\ead{email address}\n%% \\ead[url]{home page}\n%% \\fntext[label2]{}\n%% \\cortext[cor1]{}\n%% \\address{Address\\fnref{label3}}\n%% \\fntext[label3]{}\n\n\\title{ParSA: High-throughput Scientific-data Analysis Framework with Distributed File System}\n\n\n%% use optional labels to link authors explicitly to addresses:\n%% \\author[label1,label2]{}\n%% \\address[label1]{}\n%% \\address[label2]{}\n\n%\\author{Tao Zhang}\n%\\ead{t-zhang11@mails.tsinghua.edu.cn}\n%\\address{Tsinghua}\n\\author[1]{Tao Zhang\\corref{cor1}}\n\\ead{t-zhang11@mails.tsinghua.edu.cn}\n\\author[2]{XiangZheng Sun}\n%\\ead{xiangzheng.sun@intel.com}\n\\author[1]{Wei Xue}\n%\\ead{xuewei@mail.tsinghua.edu.cn}\n\\cortext[cor1]{Corresponding author. Tel: +86 18210192568}\n\\address[1]{Department of Computer Science and Technology, Tsinghua University, Beijing, China}\n\\address[2]{Intel Corporation, Beijing, China}\n\n\\begin{abstract}\nScientific-data analysis and visualization has become a key component in nowadays large-scale simulations. Due to the rapidly\nincreasing data volume and awkward I/O pattern among high-structured files, known serial methods/tools cannot scale well and \nusually lead to poor performance over traditional architectures. In this paper, we propose a new framework: ParSA \n(parallel scientific-data analysis) for high-throughput and scalable scientific analysis, with distributed file system. ParSA\npresents the optimization strategies for grouping and spliting logical units to maximize distributed I/O property of distributed \nfile system as well as to maximize overlapping the data reading, processing and transferring during computation. Besides, ParSA \nprovides the similar interfaces as the NCO (NetCDF Operator), which is used in most of climate data diagnostic packages, making\nit easy to port this framework. We utilize ParSA to accelerate well-known analysis methods for climate models on Hadoop Distributed File \nSystem (HDFS). Experimental results demonstrate the high efficiency and scalability of ParSA, getting the maximum 1.2GB/s throughput.\n\\end{abstract}\n\n\\begin{keyword}\n%% keywords here, in the form: keyword \\sep keyword\n\n%% PACS codes here, in the form: \\PACS code \\sep code\n\n%% MSC codes here, in the form: \\MSC code \\sep code\n%% or \\MSC[2008] code \\sep code (2000 is the default)\nData intensive \\sep Scientific data analysis \\sep Distributed file system\n\\end{keyword}\n\n\\end{frontmatter}\n\n\\linenumbers\n\n%% main text\n\\section{Introduction}\nIn most of modern scientific applications, huge amounts of data are produced. Large-scale simulations, such as climate modeling,\nhigh-energy physics simulation and genome mapping, generate hundreds of terabytes data volumes\\cite{kosar2009new}\\cite{hey2003data}. \nAdditionally, it still increases as the high resolution model developping. As a consequence, analysis of scientific-data is \ndata-intensive.\\par\nIn fact, almost all of scientific-data are stored in high-structured files, some of which provide parallel I/O interface, such as\nNetwork Common Data Format version 4 (NetCDF4)\\cite{rew2006netcdf}, Hierarchical Data Format 5(HDF5)\\cite{folk1999hdf5} and ADIOS BP data\nformat (BP) \\cite{lofstead2008input}, and some of which only support serial I/O interface,  like Network Common Data Format version 3\n(NetCDF3) \\cite{rew1990netcdf}. All of them are self-describing, machine-independent data format. \\par\nIn scientific-data analysis, large-scale scientific-data are stored in RAID-5/6 or parallel file system. Yet the analysis methods/\ntools are always centralized approaches, such as NCO\\cite{zender2008analysis} and NCL\\cite{ncl}, which are the most used in climate \napplications for prossing NetCDF files, leading to very poor scalability and time-consuming performance. \\par    \nInspired by big data solution in Internet Port Data Center (IPDC), numerous frameworks with distributed strategy have been developed.\nMapReduce is a program framework for processing and generating large data sets, provideing automatic parallel mechanism and build-in \nfault-tolerance on a cluster\\cite{dean2008mapreduce}. However, this solution with MapReduce requires data firstly to be transformed into \na text-based format\\cite{zhao2010parallel}. SciHadoop\\cite{buck2011scihadoop} is a Hadoop plugin allowing scientists to specify logical \nqueries over array-based data models. It executes queries as map/reduce programs defined over the logical data model. It shows remarkable \nimprovements for holistic functions of NetCDF data sets for the following optimization goals: reduce total data transfers, reduce remote \nreads and reduce unnecessary reads. Nevertheless, SciHadoop using java language leads to the compatibility problem to the existing climate \ndata analysis tools, which is written by C shell scripts, NCL and NCO commands. The SWAMP project \\cite{wang2008clustered} has provided the \nparallel NCO operations, but the reading performance is still bottleneck. \\par\nIn this paper, we propose a new framework --- Parallel Scientific-data Analysis (ParSA). We utilize the distributed I/O property with\nHadoop Distributed File System (HDFS) to improved data reading throughput. What is more, ParSA optimizes the data layout schedule\nstored in the distributed file system to overlap the data reading, processing and transferring. Besides, it provides parallel NCO\noperations, cooperating with HDFS, making it easy to use the efficient tool, without changing a lot for current climate analysis\npackage.\n\n\\section{HDFS and scientific-data analysis}\nIn this section, we will present the property of distributed file system HDFS, relicas and scheduler, which can be taken advantage\nof to optimize distributed I/O performance. We also present the character of scientific-data analysis, and discuss about the \nprobability of analysis transportaion onto HDFS.\n\n\\subsection{HDFS}\nHDFS\\cite{borthakur2008hdfs} is an open source project, driven by Google File System (GFS)\\cite{ghemawat2003google}. As a distributed, \nscalable and portable file system, HDFS is inherent for large-scale data-intensive process. \\par\nIn HDFS, there are two types of node, Namenode and Datanode. Namenode maintains file system tree and metadata for all files or\ndirectories stored in HDFS, and Datanodes are where the data are actually stored. When a file are stored into HDFS, it will be\nsplit into file blocks as the storage unit of HDFS. For achieving fault-tolerance, HDFS stores three replicas, by default, for each\nfile blocks in different datanodes. Therefore, even individual node halts down, all data, which are stored in the halted node, can\nbe accessed from other replicas. All of I/O operates can be mantipulaed through Clientnode. \\par\nEach Datanode can mount several hard disks and it manages these hard disks by itself. By default, Datanode will store each block\ninto the hard disks in a round-robin way. For example, when a file ``1'' will be stored into HDFS via Clientnode, Namenode will add\n``a'' into the file system tree. Then Clientnode begin writing the content into HDFS. Once Clientnode detects that current writing\nsize exceeds the block size, 64MB by default, it will ask Namenode for a new block with unique block number. Simultaneously,\nNamenode need recode mapping relation between ``1'' and block numbers. Since three replicas are used in HDFS for fault tolerance,\nNamenode will select three Datanodes to store a block in the file ``1''. In Datanode, HDFS should choose a hard disk to each relica \nof a block. As shown in Figure.\\ref{figure1}, the last block ``k'' is stored in disk ``2'', then the new one ``j'' will locate at disk \n``0'' according to the round-robin rule. \\par\n\n\\begin{figure}\n	\\centering\n	\\includegraphics{figure1}\n	\\caption{HDFS Architecture Block N represents $N_{th}$ file block of File ``1'' on HDFS}\n	\\label{figure1}\n\\end{figure}\n\nBlocks of files are distributed in the two-tiered storage achitechture, Datanode and Datanode's disk. It will make full use of the\ncollective bandwidth of HDFS if each replica of blocks is appropriately choosen and scheduled to reduce remoting reading among\nDatanodes. \\par\nMapReduce is usually introduced as a computation model cooperating with HDFS. It can process the data with good locality.\nHowever, due to the traits of scientific data and the operation pattern, MapReduce cannot be directly utilized on scientific-data\nanalysis with perfect performance.\n\n\\subsection{Scientific-data Analysis}\nScientific-data are usually stored in high-structured files, a kinds of spectial binary format, which can not be read directly by\nMapReduce. Each file contain multi logical units, and each unit are corresponding to its own phsical meaning. The size of \neach logical varies a lot in one file and only same logical units can be manipulated in most of analysis operations. \\par\nIn this paper, climte model data is used, the total size of which is extremely huge. One single file of CAM at 0.125 degrees,\nan atmosphere model from NCAR\\cite{neale2010description}, is 24GB size along with one single 3D variable is 616MB and one 2D variable \nis 24MB. It is 28.8TB size of 100 years of monthly output\\cite{jacob2012new}. These data are organized by thousands of files with \ntimestamp named in NetCDF format, called history result. Each file contains the same dozens of physical variables. Analysis \noperations need to process the whole or part of the history results. These operations include computing average, combining the same \nvariables in different files, producing the remapping file and fast fourier transform (FFT), etc. They require to handle whole or\npart of variables in multi-files.\n\n\\subsection{Problems and Chanllenges} \nProblem/Challenge 1: How to define the operation unit considering the storage unit？ Logical unit size varies a lot from a few bytes to\ngigabytes. Then it can not be used as the operation unit. Otherwise, the workflow of each task will be imbalance. \\par\nProblem/Challenge 2: How to schedule tasks among many files to increases the locality and reduce the data network transfer? If all logical\nunits with same meanings locate on the same Datanode, all data access will proceed locally. However, default block placement policy of HDFS\ncannot ensure that, which leads huge amount data network transfer, as the above mentioned. \\par\nProblem/Challenge 3: How to best utilize the disk I/O? There may be multi disks in one Datanode. It can not take use of disks thoughput if\nreading operations is serial or processing, reading and transferring is non-overlapping.  \n\n\\section{Parallel scientific-data analysis}\nIn order to solve the problems mentioned in previous section to take full use of distributed I/O performance, we propose a new framework \n--- Parallel Scientific-data Analysis (ParSA). \n\n\\subsection{Logical Unit Split and Group among File Blocks}\nNo matter how a file is split, all data in one block are stored contiguously in a physical location, since file block is the storage\nunit of HDFS. Thus the block of HDFS can be used as the basic operation unit to keep continuity in disk and improve I/O performance \nby proper scheduling to reduce network read. In a block, we can group the small size of logical units or split the big size one.\\par\nThe logical unit split and group approach is shown in Figure.\\ref{figure2}. All logical units located in one block are distributed \ninto one group. If a logical unit spans two or more blocks, this logical unit will split to several parts. For example, LU1 spans three \nblocks. It will split into three split parts - LU1 (1), LU1 (2) and LU1 (3), unique identifier assigned to each part. Then LU1 (1) \nand LU0 locate in file block 0 and form a operation unit group. LU1 (2) itself is distributed as a group. The remaining LU1 (3), LU2 \nand LU3 are distributed to the third group. The mapping relationship between groups and file blocks should be records, as shown in \nFigure.\\ref{figure3}. \\par\n\n\\begin{figure}\n	\\centering\n	\\includegraphics{figure2}\n	\\caption{Logical Units Spliting and Grouping into HDFS Block. $LU_N$ presents $N_{th}$ logical unit. $LU_N(i)$ stands for \n			$i_{th}$ part of $LU_N$}\n	\\label{figure2}\n\\end{figure}\n\n\\begin{figure}\n	\\centering\n	\\includegraphics{figure3}\n	\\caption{Mapping Relationship between Logical Unit Groups and File Blocks}\n	\\label{figure3}\n\\end{figure}\n\nVariables in NetCDF, used in climate data, can be viewed as multi-dimension array. An array section is defined as a contiguous rectangular\nblock specified by two vectors - index vector and count vector. The index vector indicates the start offset of the element in the corner\nclosest to the orign. The count vector gives the lengths of the edges along each of the variable's dimension. \\par\nFor instance, the variable given in Figure.\\ref{figure4a} is a two-dimension slab. If we access the whole variable, index vector is (0, 0) \nand count vector is (3, 5). However, if this variable locates on two file blocks stored with HDFS, and variable are split between value \n(1, 1) and value (1, 2), it cannot be split into these two pieces. The reason is that each of part is not a rectangular shape, which \ncan not use an index vector and count vector to represent. In this situation, we should split the variable into four pieces, as \nFigure.\\ref{figure4b} shows.\n\n\\begin{figure}[htb]\n    \\centering\n    \\begin{subfigure}{.5\\textwidth}\n        \\includegraphics[width=\\textwidth]{figure4a}\n        \\caption{}\n        \\label{figure4a}\n    \\end{subfigure}\n\n    \\begin{subfigure}{\\textwidth}\n        \\centering\n        \\includegraphics[width=.5\\textwidth]{figure4b}%\n        \\caption{}\n        \\label{figure4b}\n    \\end{subfigure}\n    \\caption{Variable Split in NetCDF. (a) Variable to be split. (b) The split result of variable given in (a). The index vectors of four\n            split slab are (0, 0), (1, 0), (1, 2) and (2, 0) respectively. And the counts vectors are (1, 5), (1, 2), (1, 3) and (1, 5) \n            respectively.}\n\\end{figure}\n\n\\subsection{Scheduler}\nAs discussed in section 2.3, quite a few operations are only executed on the same logical units among multi-files. In principle, blocks \ncontaining these units should be scheduled in one Datanode as many as possible to increase the acess locality. \\par\nAt first, information about file distribution should be gathered. As discussion in section 2.1, file block has three replicas by default. \nIn this paper, we define the tuple (Ni, Dj) as the location of a file block, which means Datanode number and hard disk number, respectively. \nAssume we get the following block location information for the first file block of all three files. \\par\n\n$$\\begin{array} {lcrr}\nF0,B0: & (N0,D0) & (N1,D2) & (N5,D1) \\\\\nF1,B0: & (N0,D1) & (N2,D0) & (N3,D1) \\\\\nF2,B0: & (N1,D2) & (N6,D0) & (N7,D1)\n\\end{array}$$\n\nFor the nine replicas for B0 of F1, F2 and F3, Datanode N0 covers 2 replicas. N1 contains 2 replicas and other Datanode only contain 1 for \neach. Then Datnode N0 and N1 contain the same number of replicas. Therefore, Datanode N0 is chosen to process the B0 block of F1 and F2,\ninstead of N1 to process B0 of F0 and F2. The reason is that B0 of F0 and F2 locates on the same hard disk in N1 Datanode, which is more \nlikely to impact the performance (explained in section 3.3). Although N1, N6 and N7 all have the B0 relica of F2, N1 is selected. \nBecause when disk 0 of N0 is broken, the relica B0 of F0 can be accessed in N1. Accroding to this principle, other blocks can be scheduled\nas the Figure.\\ref{figure5b} shown.\n\n\\begin{figure}[htb]\n    \\centering\n    \\begin{subfigure}{.5\\textwidth}\n        \\includegraphics[width=\\textwidth]{figure5a}\n        \\caption{}\n        \\label{figure5a}\n    \\end{subfigure}\n\n    \\begin{subfigure}{.5\\textwidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figure5b}%\n        \\caption{}\n        \\label{figure5b}\n    \\end{subfigure}\n    \\caption{Task Scheduler of ParSA. (a) Three files stored in HDFS. (b) Static task scheduler result of ParSA for the three files in (a). FN presents the $N_{th}$ file to be processed; BN presents $N_{th}$ file block on HDFS; (Ni, Dj) stands for corresponding file block \n    in (a) locates on $j_{th}$ disk of $i_{th}$ Datanode.}\n    \\label{figure5}\n\\end{figure}\n\n\\subsection{Workflow of ParSA}\nAlthough we utilize the file block location in HDFS to schedule which block replica are selected among Datanodes, I/O bandwidth of disks may \nnot be efficiently utilized intra-Datanode. For example, the file block B0s of F0 and F1 are assigned to N0 in Figure.\\ref{figure5}. Even\nthough B0 of F0 and B0 of F1 locate on different disks - D0 and D1 respectively, the two disks are accessed sequentially instead of \nparallelly, if the operations are implement from logical level at Datanode N0 as following:\n\n\\begin{algorithm}[htb]\n\\caption{Sequential operation on disk in the same Datanode} \n\\label{alg:sequential-operation}\n\\begin{algorithmic}\n\\FOR{each logical-unit \\textbf{LU} in B0}\n\\FOR{each file \\textbf{Fx} whose block B0 locates on N0}\n\\STATE read \\textbf{LU} from B0 of Fx\n\\STATE operate on \\textbf{LU} \n\\ENDFOR\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\nIn this paper, we propose a new workflow to efficiently access the data on HDFS and maximize utilizing all disk bandwidth in one \nDatanode. \\par\nIn ParSA, the framework of hybird of MPI and pthread is used. MPI is responsible for parallelism and communication among Datanodes. There\nis one MPI process in each Datanode. Pthead is responsible to implement the detail operations within each Datanode, constituting of \nthree major components: reading-thread, processing-thread and receiving-thread. \\par\n\\textbf{Reading-thread} If more than one thread dedicatedly access one disk, it cannot get a higher performance than only one thread. \nThus we invoke one thread for each disk. This is defined as reading-thread and use $Tread_i$ to present the $i_{th}$ reading-thread. \nFor each block, $Tread_i$ reads the queue of logical units, shown in Figure.\\ref{figure3}. After reading the logical unit, it is \nmarked as LOCAL\\_FLAG, distinguishing with receiving data. The workflow of reading-thread is given below:\n\n\\begin{algorithm}[htb]\n\\caption{reading-thread} \n\\label{alg:reading-thread}\n\\begin{algorithmic}\n\\STATE inputs:\n\\STATE \\textbf{datanode\\_number}:the Datanode where the reading-thread locates.\n\\STATE \\textbf{disk\\_number}:the disk number to be accessed. \n\\STATE \\textbf{scheduler\\_result}:task scheduler result, given in Figure 5(b).\n\\STATE \\textbf{LU\\_groups\\_map}:mapping relations between logical unit groups and file blocks in Figure 3.\n\\newline\n\\FOR{each tuple (node\\_no, disk\\_no) in \\textbf{scheduler\\_result}} \n\\IF{node\\_no = \\textbf{datanode\\_number} and disk\\_no = \\textbf{disk\\_number}}\n\\STATE get the \\textbf{file-index} and \\textbf{block-index} according to the index of tuple, shown in Figure 5(a).\n\\FOR{each \\textbf{lu} in \\textbf{LU\\_groups\\_map[block\\_index]}}\n\\STATE read \\textbf{values} of \\textbf{lu} of \\textbf{file\\_index} file\n\\STATE store \\textbf{file-index} and \\textbf{values} into \\textbf{lu}\n\\STATE mark \\textbf{lu} as \\textbf{LOCAL\\_TAG}\n\\STATE insert (\\textbf{lu}) into data queue\n\\STATE notify processing-thread\n\\ENDFOR\n\\ENDIF\n\\ENDFOR\n\\STATE send \\textbf{ending-flag} to processing-thread\n\\end{algorithmic}\n\\end{algorithm}\n\n\\textbf{Processing-thread} After reading-threads read logical units into queue, the processing-thread will be waked up. The processing-\nthread takes in charge of calling user-defined function local\\_data\\_process to process these logical units. Users only focus on \nimplementing operations on logical unit. Since each Datanode only covers part of entire dataset, the major process should reduce the \nintermediate results, computed with each process in each Datanode. This operation is another user-defined recv\\_data\\_process function. \n\n\\begin{algorithm}[htb]\n\\caption{processing-thread} \n\\label{alg:processing-thread}\n\\begin{algorithmic}\n\\STATE inputs:\n\\STATE \\textbf{num\\_tasks}:total number of \\textbf{ending-flag} to wait\n\\STATE \\textbf{intermediate\\_results}:store the intermediate results\n\\newline\n\\STATE \\textbf{cur\\_num\\_tasks} = 0\n\\WHILE{$\\textbf{cur\\_num\\_tasks} < \\textbf{num\\_tasks}$}\n\\STATE wait until receiving notification\n\\IF{receive \\textbf{ending-flag}}\n\\STATE $\\textbf{cur\\_num\\_tasks++}$\n\\STATE fetch the logical unit queue\n\\FOR{each \\textbf{lu} in logical unit queue}\n\\STATE get \\textbf{interm\\_res} according to \\textbf{lu identifier} \n\\IF{\\textbf{lu's} flag == LOCAL\\_FLAG}\n\\STATE call \\textbf{local\\_data\\_process} (\\textbf{lu}, \\textbf{interm\\_res})\n\\ENDIF\n\\IF{\\textbf{lu's} flag == RECV\\_FLAG}\n\\STATE call \\textbf{recv\\_data\\_process} (\\textbf{lu}, \\textbf{interm\\_res}) \n\\ENDIF\n\\IF{is not major process}\n\\IF{all logical unit are processed in this Datanode}\n\\STATE store \\textbf{interm\\_res} into lu\n\\STATE mark lu as \\textbf{RECV\\_TAG}\n\\STATE send (lu) to major process\n\\ENDIF\n\\ENDIF\n\\ENDFOR\n\\ENDIF\n\\ENDWHILE\n\\end{algorithmic}\n\\end{algorithm}\n\n\\textbf{Receiving-thread} Receiving-thread is a component in major process. It receives logical inits from worker processes, and\nputs the logicl units into the receiving queue. Then the processing-thread is notified to fetch these data. \\par\nThe workflow of ParSA is shown in Figure.\\ref{figure6}. Major process locates in Datanode 0, and a worker process is presented as \nDatanode 2. Each reading thread reads the logical units which is assigned to this thread. After $Tread_0$ in Datanode 2 reading the \nlogical LU1(3) of F0, it reads LU2 of F0, inserts the values into queue and notifies processing-thread to wake up. The processing-thread \nprocesses all the values in queue sequentially. When processing-thread detects that LU1(3) are all processed in Datanode 2, it will \nsend the intermediate results to the major process. Once Receiving-thread in major process gets the intermediate results, it inserts the results into the logical unit queue. Since the logical unit queue is the shared resource, all threads must access exclusively. \\par\n\n\\begin{figure}[htb]\n    \\centering\n    \\begin{subfigure}{1\\textwidth}\n        \\includegraphics[width=\\textwidth]{figure6a}\n        \\caption{}\n        \\label{figure6a}\n    \\end{subfigure}\n\n    \\begin{subfigure}{\\textwidth}\n        \\centering\n        \\includegraphics[width=1\\textwidth]{figure6b}%\n        \\caption{}\n        \\label{figure6b}\n    \\end{subfigure}\n    \\caption{Workflow of ParSA. (a) Major process; (b) Worker process. The logical units with gray background stand for already-processed\n             logical unit.}\n    \\label{figure6}\n\\end{figure}\n\n\nParSA is the framework for scientific-data access on HDFS. Users can only put their focus on logical operations. ParSA is implemented \nusing MPI and Pthread, providing more flexible configration with regard to performance than MapReduce. ParSA can achieve load-balance based\non current block replicas layout information. However, if the layout itself is unbalanced, the performance is also impacted. In the next \nsection, how to further improve the performance by optimizing layout will be discussed.\n\n\\section{Block layout optimization}\nIn section 3.2, the scheduler of ParSA utilizes the location information of file blocks. In principle, file blocks containing the same\nlogical units in the one Datanode should be scheduled as many as possible for the operations involving multi-files, while the default\nblock layout strategy of HDFS cannot guarantee this. \\par\nIn HDFS, if Clientnode is one of the Datanodes, the first replica of each file block will be stored in Clinenode, and other replicas are \nchosen random Datanodes expect for Clientnode. If the Clientnode is not Datanode, the fisrt replica will be randomly stored in a \nDatanode. \\par\nAlthough the scheduler can assign tasks based on current file block replicas distribution, the distribution should be control according \nto the principle for task assignment in order to achieve load balancing. Since blocks of files are stored in Datanode and Datanode's disk,\nit needs to optimize the layout in these the two tiered architecture. \n\n\\subsection{Data layout inter-Datanodes}\nTaken into account the characteristic of scientific-data analysis, tightly coupled data as above described should be assigned in one \nDatanode to increase locality of processing data. Simultaneously, each Datanode should have equal workload, with a pretty load-balance \nperformance. \\par \nHowever, it may indeed exits a few analysis operations different from the characteristic. ParSA expose an interface for users to determine\nwhere the first replicas of blocks should locate. What's more, the second replicas are stored with uniform distribution automaticlly.\nThe third abide by the HDFS default strategy.  \n\n\\subsection{Data layout intra-Datanodes}\nAs discussed in section 3.3, the workflow of ParSA invokes one thread for one disk. If blocks are imbalance among disks in one Datanode, \nit will waste the bandwidth of other disks. The load balance is not only the equal number of file blocks per disk in one Datanode, but \nalso the distribution of blocks containing the same logical units. The former can be guaranteed by HDFS default disk allocation strategy\nwith a round-robin way, while the later can not be satisfied all the time. ParSA provides a round-robin mechanism for these blocks \ndistribution, as shown in Figure.\\ref{figure7}. The disk will be used for a block, if it does not store one with the same logical \nunits. \\par\n\n\\begin{figure}[htb]\n    \\centering\n    \\begin{subfigure}{0.5\\textwidth}\n        \\includegraphics[width=\\textwidth]{figure7a}\n        \\caption{}\n        \\label{figure7a}\n    \\end{subfigure}\n\n    \\begin{subfigure}{\\textwidth}\n        \\centering\n        \\includegraphics[width=0.5\\textwidth]{figure7b}%\n        \\caption{}\n        \\label{figure7b}\n    \\end{subfigure}\n    \\caption{Round-robin Method for File Block with Same Meaning. (a) Block layout unbalance for B0. There are two B0s locate on Disk 0\n             while Disk 2 contains no B0. (b) Using the improved strategy. B0s are stored in a round-robin way and locate evenly among \n             disks.}\n    \\label{figure7}\n\\end{figure}\n\n\\section{Evalution}\n\n\\subsection{Experimental setting}\nTo demonstrate the performance portability of ParSA framework, we choose two HDFS clusters with different configuration, as shown in Table\n.\\ref{table1} and Table.\\ref{table2}. \\par\n\n\\begin{table}[ht]\n\\caption{System Configuration of hadoop cluster 1} \n\\centering \n\\begin{tabular}{c c}\n\\hline\nconfiguration & parameter \\\\\n\\hline \n\\#nodes & 16 \\\\ \n\\#disks per node & 2x1 TB SATA2 \\\\\nOS & RedHat 6.2 \\\\\nMPI & Intel® MPI Library 3.2 \\\\\nCompiler & Intel® Composer XE 2011 \\\\\nNetWork & Infiniband QDR \\\\\nCPU & Intel® Xeon® Processor E5620 @2.4GHz \\\\\nMemory & 48GB \\\\\nHadoop & Apache Hadoop 1.0.3 \\\\\n\\hline \n\\end{tabular}\n\\label{table1} \n\\end{table}\n\n\\begin{table}[ht]\n\\caption{System Configuration of hadoop cluster 2} \n\\centering \n\\begin{tabular}{c c}\n\\hline\nconfiguration & parameter \\\\\n\\hline \n\\#nodes & 6 \\\\ \n\\#disks per node & 5x500 GB SAS \\\\\nOS & RedHat 6.2 \\\\\nMPI & Intel® MPI Library 4.1 \\\\\nCompiler & Intel® Composer XE 2011 \\\\\nNetWork & Dual GigE Ethernet \\\\\nCPU & Intel® Xeon® Processor E5645 @2.4GHz \\\\\nMemory & 48GB \\\\\nHadoop & Apache Hadoop 1.0.3 \\\\\n\\hline \n\\end{tabular}\n\\label{table2} \n\\end{table}\n\nIn order to demonstrate the efficiency of ParSA, we use climate data generated by a ocean model, POP, a component of climate model. The \ndataset has 100 NetCDF files, and each of file contains 60 two-dimension variables (logical units) and 18 three-dimension variables, with\n320x384 and 320x384x40 resolution, which has 309 MB datasize. The total size of this dataset is 30.9 GB. \\par \nThe major analysis operation is calculating time average using ncea command of NCO package, which is the basic preprocessing operation\nfor almost all of climate data analysis. \\par\nTo evaluate the scalability of ParSA, we compare the speedup and throughput performance of different number of Datanodes and different\nnumber of disks per node. Cluster 1 is used for the former experiment since the number nodes is more than cluster 2. While \ncluser 2 is used for the later experiment since each node has more disks in cluster 1. \\par \n\n\\subsection{Scalability with different disks per Datanode}\nParSA can get about 1.3 GB/s aggregation throuphout if per Datanode uses 5 disks, as shown Figure.\\ref{figure8a}. The throughput has a \nremarkable scalability if per Datanode uses 1 to 4 disks. However, it has little performance improvement with 5 disks. The reason is \nthat there are some serial operations, such as writing results and communication, which impact on the performance. Since a series of \nstrategies propused by ParSA is aimed at I/O, Figure.\\ref{figure8b} shows the speedup with only reading part. It's scalability is close\nto the ideal speedup. \n\n\\begin{figure}[tbh]\n    \\centering\n    \\begin{subfigure}{0.5\\textwidth}\n        \\includegraphics[width=\\textwidth]{figure8a}\n        \\caption{}\n        \\label{figure8a}\n    \\end{subfigure}\\begin{subfigure}{0.5\\textwidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figure8b}%\n        \\caption{}\n        \\label{figure8b}\n    \\end{subfigure}\n    \\caption{Scalability of ParSA along with different number of disks per Datanode. (a) Throughput; (b) Speedup.}\n    \\label{figure8}\n\\end{figure}\n\n\\subsection{Scalability with different Datanodes}\nTwo disks per node are used in this evaluation with cluster 1. It almost keeps the linear speedup until 9 Datanodes as shown Figure 9. \nHowever, the performance improvment is limited when the Datanodes are greater than 9 due to the increasing overhead of communication\nand the critical section controlled by mutex, as shown Figure.\\ref{figure6}. Although data processing is overlapped with data accessing, \nthe time non-overlapped occupies almost 38.89\\% on 16 datanodes, and this proportions are only 0.02\\% and 7.96\\% when datanodes are 9 \nand 12 respectively. \\par\nIn Figure.\\ref{figure9a}, it can get about 800MB/s maximum throughput on 12 datanodes, with 24 disks totally. While the same number of \ndisks in Figure.\\ref{figure8a}, 4 disks per datanode, it can get about 1.2GB/s. The same performance only need 2 disks per datanode, \nwith 12 disks totally. The reason is that, on one hand, the disks and network are difference between the two cluters; on the other hand,\ncluster 1 requires more network access due to the few disks per node.  \\par\nThe speedup of only reading is presented as Figure.\\ref{figure9b}, which shows an exllcent scalability with different datanodes. The \nreason why the speedup is higher than the ideal may result from performance variability. \n\n\\begin{figure}[tbh]\n    \\centering\n    \\begin{subfigure}{0.5\\textwidth}\n        \\includegraphics[width=\\textwidth]{figure9a}\n        \\caption{}\n        \\label{figure9a}\n    \\end{subfigure}\\begin{subfigure}{0.5\\textwidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figure9b}%\n        \\caption{}\n        \\label{figure9b}\n    \\end{subfigure}\n    \\caption{Scalability of ParSA along with different number of Datanodes. (a) Throughput; (b) Speedup.}\n    \\label{figure9}\n\\end{figure}\n\n\\subsection{Throughput comparision with different task optimization strategies}\nHDFS block is the basic operation unit in ParSA, spliting and grouping the logical unit. We select the block replicas in different \nDatanodes to reduce network reading, optimize the workflow to support overlapping and parallelism of reading and processing, as well as \nthe optimization of blocks layout for both inter-datanodes and intra-datanodes. \\par\nTo present the improving performance, two additional strategies are compared to ParSA. The first one takes logical unit as the operation\nunit. The disk utilization is shown as Figure.\\ref{figure10a}. Since the size of logical unit varies quite a lot, a serious load imbalance\nexists among tasks. The second strategy takes all logical units located in one file block as an operation unit, while those logical unit \nwhich locate on more than one block do not be split and treat the big size logical units as other operation unit. In this situation, it can\nnot guarantee that an entire operation unit stores in a disk. When a thread try to read this unit, it requires read a part of value from the\nother disk. In the mean time, the other thread is reading the same disk, leading to disk resource competition. This utilization performance\nis presented as Figure.\\ref{figure10b}. \\par\nCompared with throuphput to evaluate the speedup, ParSA outperforms the above two strategies by a factor of 3.34X and 1.14X respectively.\n\n\\begin{figure}[tbh]\n    \\centering\n    \\begin{subfigure}{0.5\\textwidth}\n        \\includegraphics[width=\\textwidth]{figure10a}\n        \\caption{}\n        \\label{figure10a}\n    \\end{subfigure}\\begin{subfigure}{0.5\\textwidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figure10b}%\n        \\caption{}\n        \\label{figure10b}\n    \\end{subfigure}\n    \\begin{subfigure}{0.5\\textwidth}\n        \\centering\n        \\includegraphics[width=\\textwidth]{figure10c}%\n        \\caption{}\n        \\label{figure10c}\n    \\end{subfigure}\n    \\caption{Comparison of Disk-utilization with Different Task Assignment Strategies. (a) Logicl Unit. (b) Block Unit without Splitting. \n             (c) Strategy of ParSA}\n    \\label{figure10}\n\\end{figure}\n\n\\section{Conclusion and future work}\nA high-throughput and scalable scientific framework combined with HDFS distributed file system and NCO analysis tool has been designed, \nimplemented and evaluated. Maximize distributed I/O performance has been gained by scheduling block repicas in different datanodes, \noptimizing the blocks layout for both inter-datanodes and intra-datanodes,  as well as overlapping the data reading, processing and \ntransferring. Instead of Hadoop framework, we use multi-process with MPI to parallel the NCO basic operations, without leading to compatible \nproblem. In future work, we plan on testing the framework on some complex climate data analysis, such as FFT, principal component analysis \nand Wavelet transform.\n\n%\\label{text}\n	\n%% The Appendices part is started with the command \\appendix;\n%% appendix sections are then done as normal sections\n%% \\appendix\n\n%% \\section{}\n%% \\label{}\n\n%% If you have bibdatabase file and want bibtex to generate the\n%% bibitems, please use\n%%\n\\bibliographystyle{elsarticle-num} 	\n\\bibliography{zhangtao}\n\n%% else use the following coding to input the bibitems directly in the\n%% TeX file.\n\n%\\begin{thebibliography}{00}\n\n%\\bibitem{label}\n%% Text of bibliographic item\n\n%\\bibitem{text}\n\n%\\end{thebibliography}\n\n\\end{document}\n\\endinput\n%%\n%% End of file `elsarticle-template-num.tex'.\n",
			"file": "elsarticle-template-num.tex",
			"file_size": 37304,
			"file_write_time": 1402865204000000,
			"settings":
			{
				"buffer_size": 37295,
				"line_ending": "Unix"
			}
		},
		{
			"file": "zhangtao.bib",
			"settings":
			{
				"buffer_size": 4524,
				"line_ending": "Unix",
				"name": "zhangtao.bib"
			}
		},
		{
			"file": "elsarticle.dtx",
			"settings":
			{
				"buffer_size": 27201,
				"line_ending": "Unix"
			}
		},
		{
			"file": "elsarticle-template-num.synctex.gz",
			"settings":
			{
				"buffer_size": 285305,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"command_palette":
	{
		"height": 87.0,
		"selected_items":
		[
			[
				"push",
				"Git: Push"
			],
			[
				"comm",
				"Git: Commit"
			],
			[
				"add",
				"Git: Add..."
			],
			[
				"",
				"Git: Add..."
			],
			[
				"install",
				"Package Control: Install Package"
			],
			[
				"git add",
				"Git: Add..."
			],
			[
				"git comm",
				"Git: Commit"
			],
			[
				"git sta",
				"Git: Status"
			],
			[
				"git",
				"Git: Status"
			],
			[
				"git push",
				"Git: Push"
			],
			[
				"git ",
				"Git: Status"
			],
			[
				"stas",
				"Git: Status"
			],
			[
				"git co",
				"Git: Commit"
			],
			[
				"commit",
				"Git: Commit"
			],
			[
				"pus",
				"Git: Push"
			],
			[
				"git init",
				"Markdown Preview: Github Flavored Markdown: Export HTML in Sublime Text"
			],
			[
				"init",
				"Indentation: Convert to Spaces"
			],
			[
				"git st",
				"Git: Status"
			],
			[
				"com",
				"Git: Commit"
			],
			[
				"git pu",
				"Git: Push"
			],
			[
				"git com",
				"Git: Commit"
			],
			[
				"git ad",
				"Git: Add Current File"
			],
			[
				"git  add",
				"Git: Add..."
			],
			[
				"la",
				"LaTeXTools: Reconfigure and migrate settings"
			],
			[
				"remove ",
				"Package Control: Remove Package"
			],
			[
				"un",
				"Snippet: Underline text"
			],
			[
				"mar",
				"Markdown Preview: Github Flavored Markdown: Preview in Browser"
			],
			[
				"markdow",
				"Markdown Preview: Github Flavored Markdown: Preview in Browser"
			],
			[
				"pack",
				"Package Control: Install Package"
			],
			[
				"parem",
				"Package Control: Remove Package"
			],
			[
				"par",
				"Package Control: Remove Package"
			],
			[
				"githu",
				"GitHub Flavored Markdown: Preview"
			],
			[
				"mark",
				"GitHub Flavored Markdown: Preview"
			],
			[
				"pa",
				"Package Control: Install Package"
			],
			[
				"pac",
				"Package Control: Install Package"
			],
			[
				"git:add",
				"Git: Add..."
			],
			[
				"git:",
				"Git: Status"
			],
			[
				"Package Control: install",
				"Package Control: Install Package"
			],
			[
				"Package Control:install",
				"Package Control: Install Package"
			]
		],
		"width": 647.0
	},
	"console":
	{
		"height": 125.0
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"file_history":
	[
		"/Users/zhangtao/Documents/work/2014work/fgcs/elsarticle/elsarticle-template-num.spl",
		"/Users/zhangtao/Documents/work/2014work/fgcs/elsarticle/figure6.pdf",
		"/Users/zhangtao/Documents/work/2014work/fgcs/elsarticle/test.tex",
		"/Users/zhangtao/Documents/work/2014work/fgcs/elsarticle/test.aux",
		"/Users/zhangtao/Documents/work/2014work/fgcs/elsarticle/elsarticle-template-harv.tex",
		"/Users/zhangtao/Library/Application Support/Sublime Text 2/Packages/Alignment/Base File.sublime-settings",
		"/Users/zhangtao/Downloads/latex.tex",
		"/Users/zhangtao/Downloads/test.tex",
		"/Users/zhangtao/Library/Application Support/Sublime Text 2/Packages/User/LaTeXTools.sublime-settings",
		"/Users/zhangtao/Library/Application Support/Sublime Text 2/Packages/LaTeXTools/README.markdown",
		"/Users/zhangtao/Documents/nfs/movie/Untitled Document",
		"/Volumes/zhangtao/data/TH_1A_testing_2014.5.5/bp2ncd.c",
		"/Users/zhangtao/Library/Containers/com.apple.mail/Data/Library/Mail Downloads/CF24E071-EE50-47B9-A608-0DD6F2DC43D9/skel_apps/s3d/s3d.xml",
		"/Users/zhangtao/Downloads/licom_skel.c",
		"/Users/zhangtao/Downloads/licom2.xml",
		"/Users/zhangtao/Downloads/Makefile",
		"/Users/zhangtao/Documents/work/sslvpn_jnlp.cgi.jnlp",
		"/Users/zhangtao/Documents/work/代理模式/因子数据.DAT",
		"/Users/zhangtao/Documents/hpc/TsingHpc/开题/开题notes",
		"/Users/zhangtao/Documents/work/精度文章/precision",
		"/Users/zhangtao/Dropbox/work/precision",
		"/Users/zhangtao/Dropbox/work/1. Li J D, Duan Q Y, Gong W, et al. Assessing para",
		"/Users/zhangtao/Dropbox/work/precision paper/reference/reference note",
		"/Users/zhangtao/Dropbox/work/模式的模拟与预测和评估的检验方法更加突出定量化，强调“模式的评估标准”(metric), 要对多模式",
		"/Users/zhangtao/Dropbox/work/气候系统模式参数不确定性的量化分析和优化",
		"/Users/zhangtao/Dropbox/work/UQ/referance paper/referance node"
	],
	"find":
	{
		"height": 35.0
	},
	"find_in_files":
	{
		"height": 0.0,
		"where_history":
		[
			""
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"ref",
			"acknow",
			"renewenvironment",
			"ref",
			"abstract",
			"reference",
			"thebibliography",
			"bib",
			"bi'b",
			"round",
			"two",
			"tired",
			"algori",
			"newtheorem",
			"and",
			"newtheorem",
			"problem",
			"mant",
			"man",
			"mant",
			"man",
			"li'ne",
			"nuclear",
			"pif",
			"pi",
			"nlocal",
			"LN"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 3,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "elsarticle-template-num.tex",
					"settings":
					{
						"buffer_size": 37295,
						"regions":
						{
						},
						"selection":
						[
							[
								37016,
								37016
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/LaTeX.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 13274.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "zhangtao.bib",
					"settings":
					{
						"buffer_size": 4524,
						"regions":
						{
						},
						"selection":
						[
							[
								4523,
								4523
							]
						],
						"settings":
						{
							"auto_name": "zhangtao.bib",
							"syntax": "Packages/LaTeX/Bibtex.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2189.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "elsarticle.dtx",
					"settings":
					{
						"buffer_size": 27201,
						"regions":
						{
						},
						"selection":
						[
							[
								23831,
								23834
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 14616.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "elsarticle-template-num.synctex.gz",
					"settings":
					{
						"buffer_size": 285305,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 0.0
	},
	"input":
	{
		"height": 31.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 145.0
	},
	"output.git":
	{
		"height": 142.0
	},
	"output.unsaved_changes":
	{
		"height": 136.0
	},
	"replace":
	{
		"height": 0.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"selected_items":
		[
		],
		"width": 0.0
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 270.0,
	"status_bar_visible": true
}
