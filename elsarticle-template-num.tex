%% 
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
\usepackage{lineno}

\journal{Future Generation Computer Systems}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{ParSA: High-throughput Scientific-data Analysis Framework with Distributed File System}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \address[label1]{}
%% \address[label2]{}

\author{Tao Zhang}

\address{Tsinghua}

\begin{abstract}
Scientific-data analysis and visualization has become a key component in nowadays large-scale simulations. Due to the rapidly
increasing data volume and awkward I/O pattern among high-structured files, known serial methods/tools cannot scale well and 
usually lead to poor performance over traditional architectures. In this paper, we propose a new framework: ParSA 
(parallel scientific-data analysis) for high-throughput and scalable scientific analysis, with distributed file system. ParSA
present the optimization strategies oriented for physical disk to maximize distributed I/O property of distributed file system as
well as to maximize overlapping the data reading, processing and transferring during computation. Besides, ParSA provides the similar interfaces as the NCO (netCDF Operator), which is used in most of climate data diagnostic package, making
it easy to port this framework. We utilize ParSA to accelerate well-known analysis methods for climate models on Hadoop Distributed File System (HDFS). Experimental results demonstrate the high efficiency and scalability of ParSA. 
\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
Data intensive \sep Scientific data analysis \sep Distributed file system
\end{keyword}

\end{frontmatter}

\linenumbers

%% main text
\section{Introduction}
In most of modern scitific applications, huge amounts of data are produced. Large-scale simulations, such as climate modeling,
high-energy physics simulation and genome mapping, generate hundreds of terabytes data volumes(Tevfik, 2009; Hey, 2003). 
Additionally, it still increases as the high resolution model developping. As a consequence, analysis of scientific-data is 
data-intensive.\par
In fact, almost all of scientific-data are stored in high-structured files, some of which provide parallel I/O interface, such as
Network Common Data Format version 4(NetCDF4), Hierarchical Data Format 5(HDF5) (hdfs) and ADIOS BP data format (bp) (bp), and some
of which only support serial I/O interface,  like Network Common Data Format version 3(NetCDF3). All of these are self-describing,
machine-independent data format. \par
In scientific-data analysis, large-scale scientific-data are stored in RAID-5/6 or parallel file system. Yet the analysis methods/
tools are always centralized approaches, such as NCO and NCL, which are the most used in climate applications for prossing NetCDF
files, leading to very poor scalability and time-consuming performance. \par    
Inspired by big data solution in Internet Port Data Center (IPDC), numerous frameworks with distributed strategy have been developed.
MapReduce is a program framework for processing and generating large data sets, provideing automatic parallel mechanism and build-in
fault-tolerance on a cluster. However, this solution with MapReduce requires that data first be transformed into a text-based format.
SciHadoop is a Hadoop plugin allowing scientists to specify logical queries over array-based data models. It executes queries as
map/reduce programs defined over the logical data model. It shows remarkable improvements for holistic functions of NetCDF data sets
for the following optimization goals: reduce total data transfers, reduce remote reads and reduce unnecessary reads. Nevertheless,
SciHadoop using java language leads to the compatibility problem to the existing climate data analysis tools, which is written by c
shell scripts, NCL and NCO commands. The SWAMP project [9] has provided the parallel NCO operations, but the reading performance is
still bottleneck. \par
In this paper, we propose a new framework - Parallel Scientific-data Analysis (ParSA). We utilize the distributed I/O property with
Hadoop Distributed File System (HDFS) to improved data reading throughput. What is more, ParSA optimizes the data layout schedule
stored in the distributed file system to overlap the data reading, processing and transferring. Besides, it provides parallel NCO
operations, cooperating with HDFS, making it easy to use the efficient tool, without changing a lot for current climate analysis
package.

\section{HDFS and scientific-data analysis}
In this section, we will present the property of distributed file system HDFS, relicas and scheduler, which can be taken advantage
of to optimize distributed I/O performance. We also present the character of scientific-data analysis, and discuss about the 
probability of analysis transportaion onto HDFS.

\subsection{HDFS}
HDFS is an open source project, driven by Google File System (GFS). As a distributed, scalable and portable file system, HDFS is
inherent for large-scale data-intensive process. \par
In HDFS, there are two types of node, Namenode and Datanode. Namenode maintains file system tree and metadata for all files or
directories stored in HDFS, and Datanodes are where the data are actually stored. When a file are stored into HDFS, it will be
split into file blocks as the storage unit of HDFS. For achieving fault-tolerance, HDFS stores three replicas, by default, for each
file blocks in different datanodes. Therefore, even individual node halts down, all data, which are stored in the halted node, can
be accessed from other replicas. All of I/O operates can be mantipulaed through Clientnode. \par
Each Datanode can mount several hard disks and it manages these hard disks by itself. By default, Datanode will store each block
into the hard disks in a round-robin way. For example, when a file ``a'' will be stored into HDFS via Clientnode, Namenode will add
``a'' into the file system tree. Then Clientnode begin writing the content into HDFS. Once Clientnode detects that current writing
size exceeds the block size, 64MB by default, it will ask Namenode for a new block with unique block number. Simultaneously,
Namenode need recode mapping relation between ``a'' and block numbers. Since three replicas are used in HDFS for fault tolerance,
Namenode will select three Datanodes to store a block in the file ``a''. In Datanode, HDFS should choose a hard disk to each relica 
of a block. As shown in Figure 1, the last block ``k'' is stored in disk ``2'', then the new one ``j'' will locate at disk ``0'' 
according to the round-robin rule. \par
Blocks of files are distributed in the two-tiered storage achitechture, Datanode and Datanode's disk. It will make full use of the
collective bandwidth of HDFS if each relca of blocks is appropriately choosen and scheduled to reduce remoting reading among
Datanodes. \par
MapReduce is usually introduced as a computaion model cooperating with HDFS. It can process the data with good locality.
However, due to the traits of scientific data and the operation pattern, MapReduce cannot be directly utilized on scientific-data
analysis with perfect performance.

\subsection{Scientific-data Analysis}
Scientific-data are usually stored in high-structured files, a kinds of spectial binary format, which can not be read directly by
MapReduce. Each file contain multi logical units, and each unit are corresponding to its own phsical meaning. The size of 
each logical varies a lot in one file and only same logical units can be manipulated in most of analysis operations \par
In this paper, climte model data is used, the total size of which is extremely huge. The high resolution ocean model, a sub-component
model in climate model, has 48TB for 100 simulated years, with monthly output. These data are organized by thousands of files with 
timestamp named in NetCDF format, called history result. Each file contains the same dozens of physical variables. Analysis 
operations need to process the whole or part of the history results. These operations include computing average, combining the same 
variables in different files, producing the remapping file and fast Fourier transform (FFT), etc. They require to handle whole or
part of variables in multi-files.

\subsection{Problems and Chanllenges} 
Problem/Challenge 1: How to define the operation unit considering the storage unitï¼Ÿ Logical unit size varies a lot from a few bytes to
gigabytes. Then it can not be used as the operation unit. Otherwise, the workflow of each task will be imbalance. \par
Problem/Challenge 2: How to schedule tasks among many files to increases the locality and reduce the data network transfer? If all logical
units with same meanings locate on the same Datanode, all data access will proceed locally. However, default block placement policy of HDFS
cannot ensure that, which leads huge amount data network transfer, as the above methtioned. \par
Problem/Challenge 3: How to best utilize the disk I/O?

\section{Parallel scientific-data analysis}
In order to solve the problems mentioned in previous section to take full use of distributed I/O performance, we propose a new framework - 
Parallel Scientific-data Analysis (ParSA). 

\subsection{Logical Unit Split and Group among File Blocks}
No matter how a file is split, all data in one block are stored contiguously in a physical location, since file block is the storage
unit of HDFS. Thus the block of HDFS can be used as the basic operation unit to keep continuity in disk and improve I/O performance 
by proper scheduling to reduce network read. In a block, we can group the small size of logical units or split the big size one.\par
The logical unit split and group approach is shown in Figure 2. All logical units located in one block are distributed into one group. 
If a logical unit spans two or more blocks, this logical unit will split to several parts. For example, LU1 spans three blocks. It will 
split into three split parts - LU1 (1), LU1 (2) and LU1 (3), unique identifier assigned to each part. Then LU1 (1) and LU0 locate in file
block 0 and form a operation unit group. LU1 (2) itself is distributed as a group. The remaining LU1 (3), LU2 and LU3 are distributed to the 
third group. The mapping relationship between groups and file blocks should be records, as shown in Figure 3. \par
Variables in NetCDF, used in climate data, can be viewed as multi-dimension array. An array section is defined as a contiguous rectangular
block specified by two vectors - index vector and count vector. The index vector indicates the start offset of the element in the corner
closest to the orign. The count vector gives the lengths of the edges along each of the variable's dimension. \par
For instance, the variable given in Figure 4 (a) is a two-dimension slab. If we access the whole variable, index vector is (0, 0) and 
count vector is (3, 5). However, if this variable locates on two file blocks stored with HDFS, and variable are split between value (1, 1) 
and value (1, 2), it cannot be split into these two pieces. The reason is that each of part is not a rectangular shape, which can not use
an index vector and count vector to represent. In this situation, we should split the variable into four pieces, as Figure 4(b) shows.

\subsection{Scheduler}
As discussed in section 2.3, quite a few operations are only executed on the same logical units among multi-files. In principle, blocks 
containing these units should be scheduled in one Datanode as many as possible to increase the acess locality. \par
At first, information about file distribution should be gathered. As discussion in section 2.1, file block has three replicas by default. 
In this paper, we define the tuple (Ni, Dj) as the location of a file block, which means Datanode number and hard disk number, respectively. 
Assume we get the following block location information for the first file block of all three files. \par
For the nine replicas for B0 of F1, F2 and F3, Datanode N0 covers 2 replicas. N1 contains 2 replicas and other Datanode only contain 1 for 
each. Then Datnode N0 and N1 contain the same number of replicas. Therefore, Datanode N0 is chosen to process the B0 block of F1 and F2,
instead of N1 to process B0 of F0 and F2. The reason is that B0 of F0 and F2 locates on the same hard disk in N1 Datanode, which is more 
likely to impact the performance (explained in section 3.3). Although N1, N6 and N7 all have the B0 relica of F2, N1 is selected. 
Because when disk 0 of N0 is broken, the relica B0 of F0 can be accessed in N1. Accroding to this principle, other blocks can be scheduled
as the Figure 5(b) shown.

\subsection{Workflow of ParSA}
 

%\label{text}
	
%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

\begin{thebibliography}{00}

%% \bibitem{label}
%% Text of bibliographic item

\bibitem{text}

\end{thebibliography}
\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
